{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Cse674Proj2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "Ie5qkI9nK2LX",
        "colab_type": "code",
        "outputId": "88e72fed-6f93-499d-9845-eae5fe651b93",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install pgmpy"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pgmpy in /usr/local/lib/python3.6/dist-packages (0.1.7)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from pgmpy) (1.14.6)\n",
            "Requirement already satisfied: networkx<1.12,>=1.11 in /usr/local/lib/python3.6/dist-packages (from pgmpy) (1.11)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from pgmpy) (1.1.0)\n",
            "Requirement already satisfied: decorator>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from networkx<1.12,>=1.11->pgmpy) (4.4.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3tJzj_sOLAwa",
        "colab_type": "code",
        "outputId": "3cd5e9fd-cd3e-4965-9cce-c699de03c626",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "'''from google.colab import files\n",
        "uploaded = files.upload()'''"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'from google.colab import files\\nuploaded = files.upload()'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "metadata": {
        "id": "CbXeNy9gEzdx",
        "colab_type": "code",
        "outputId": "5d3c28ca-d117-4966-d762-3a15e38879d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/kumarankit1996/Dataset.git"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'Dataset' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "fFy6R7nmGY3b",
        "colab_type": "code",
        "outputId": "649c3060-3fa0-4307-98b5-0b2d2407f354",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "cell_type": "code",
      "source": [
        "!ls Dataset/AML_Proj2/"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " 15features.csv\t\t\t  feature_Unseen_weights_v1.h5\n",
            " 31features_seen_train.csv\t 'Icon'$'\\r'\n",
            " 31_Seen_Predict_Val.csv\t  seen-dataset\n",
            " 31_Seen_Train.csv\t\t  seen_predict_feature.csv\n",
            " 31_Seen_Val.csv\t\t  seen_weights_v1_e10k.h5\n",
            " 31_shuffled_Predict_Val.csv\t  shuffled-dataset\n",
            " 31_shuffled_Train.csv\t\t  shuffled_predict_feature.csv\n",
            " 31_shuffled_Val.csv\t\t  shuffled_weights_v1_e10k.h5\n",
            " 31_unSeen_Predict_Val.csv\t  unseen-dataset\n",
            " 31_unSeen_Train.csv\t\t  unseen_predict_feature.csv\n",
            " 31_unSeen_Val.csv\t\t  Unseen_weights_v1_1.h5\n",
            " feature_seen_weights_v1.h5\t  Unseen_weights_v1_e10k.h5\n",
            " feature_shuffled_weights_v1.h5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6jtvGIOgMtcU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Import Library"
      ]
    },
    {
      "metadata": {
        "id": "vDi-8-jULodo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "import io\n",
        "from keras.layers import concatenate\n",
        "import random\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Conv2D, MaxPooling2D, BatchNormalization, Activation, Reshape, UpSampling2D,Flatten, Dropout\n",
        "from IPython.display import SVG\n",
        "from keras.utils.vis_utils import model_to_dot\n",
        "from keras.layers import Dense\n",
        "from keras import optimizers\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import os\n",
        "from google.colab.patches import cv2_imshow\n",
        "import math\n",
        "import timeit"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bS2atS3AMwaq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Import PGM library"
      ]
    },
    {
      "metadata": {
        "id": "5cJFeGqXLLBY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from pgmpy.models import BayesianModel\n",
        "from pgmpy.factors.discrete import TabularCPD\n",
        "from pgmpy.sampling import BayesianModelSampling\n",
        "from pgmpy.estimators import K2Score\n",
        "from pgmpy.inference import VariableElimination\n",
        "from timeit import default_timer as timer\n",
        "from pgmpy.estimators import ExhaustiveSearch\n",
        "from pgmpy.estimators import HillClimbSearch\n",
        "from pgmpy.estimators import BayesianEstimator"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ftojd0mvMzqr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Reading datasets"
      ]
    },
    {
      "metadata": {
        "id": "5wAZO3lMLpzu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# read csv dataset\n",
        "features=pd.read_csv(\"Dataset/AML_Proj2/15features.csv\")\n",
        "#removing unwanted columns\n",
        "#features=features.drop([\"imagename\"],axis=1)\n",
        "features.set_index('imagename',inplace=True)\n",
        "all_nodes=['pen_pressure1', 'letter_spacing1', 'size1', 'dimension1', 'is_lowercase1',\n",
        "       'is_continuous1', 'slantness1', 'tilt1', 'entry_stroke_a1', 'staff_of_a1',\n",
        "       'formation_n1', 'staff_of_d1', 'exit_stroke_d1', 'word_formation1',\n",
        "       'constancy1','pen_pressure2', 'letter_spacing2', 'size2', 'dimension2', 'is_lowercase2',\n",
        "       'is_continuous2', 'slantness2', 'tilt2', 'entry_stroke_a2', 'staff_of_a2',\n",
        "       'formation_n2', 'staff_of_d2', 'exit_stroke_d2', 'word_formation2',\n",
        "       'constancy2','label']\n",
        "feature_nodes=['pen_pressure1', 'letter_spacing1', 'size1', 'dimension1', 'is_lowercase1',\n",
        "       'is_continuous1', 'slantness1', 'tilt1', 'entry_stroke_a1', 'staff_of_a1',\n",
        "       'formation_n1', 'staff_of_d1', 'exit_stroke_d1', 'word_formation1',\n",
        "       'constancy1','pen_pressure2', 'letter_spacing2', 'size2', 'dimension2', 'is_lowercase2',\n",
        "       'is_continuous2', 'slantness2', 'tilt2', 'entry_stroke_a2', 'staff_of_a2',\n",
        "       'formation_n2', 'staff_of_d2', 'exit_stroke_d2', 'word_formation2',\n",
        "       'constancy2']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mNP0hH-3V2BD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "important=np.zeros((15,15))\n",
        "# finding correlation of features\n",
        "c=features.corr()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "c_kA4WNAf9qL",
        "colab_type": "code",
        "outputId": "fb5387aa-b6f2-4b1d-a2d5-da19bcb1cdd2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 534
        }
      },
      "cell_type": "code",
      "source": [
        "for i in range (0,15):\n",
        "    for j in range(0,15):\n",
        "        if c.iloc[i,j]<0.1:\n",
        "            # if less than threshold then less correlation\n",
        "            important[i][j]=0\n",
        "        else:\n",
        "            if i!=j:\n",
        "              important[i][j]=1\n",
        "df=pd.DataFrame.from_records(important,columns=feature_nodes[:15])\n",
        "df"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pen_pressure1</th>\n",
              "      <th>letter_spacing1</th>\n",
              "      <th>size1</th>\n",
              "      <th>dimension1</th>\n",
              "      <th>is_lowercase1</th>\n",
              "      <th>is_continuous1</th>\n",
              "      <th>slantness1</th>\n",
              "      <th>tilt1</th>\n",
              "      <th>entry_stroke_a1</th>\n",
              "      <th>staff_of_a1</th>\n",
              "      <th>formation_n1</th>\n",
              "      <th>staff_of_d1</th>\n",
              "      <th>exit_stroke_d1</th>\n",
              "      <th>word_formation1</th>\n",
              "      <th>constancy1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    pen_pressure1  letter_spacing1  size1  dimension1  is_lowercase1  \\\n",
              "0             0.0              1.0    0.0         0.0            0.0   \n",
              "1             1.0              0.0    0.0         0.0            0.0   \n",
              "2             0.0              0.0    0.0         1.0            0.0   \n",
              "3             0.0              0.0    1.0         0.0            0.0   \n",
              "4             0.0              0.0    0.0         0.0            0.0   \n",
              "5             0.0              0.0    0.0         0.0            1.0   \n",
              "6             0.0              0.0    0.0         0.0            0.0   \n",
              "7             0.0              0.0    0.0         0.0            0.0   \n",
              "8             0.0              0.0    0.0         0.0            0.0   \n",
              "9             0.0              0.0    0.0         0.0            1.0   \n",
              "10            0.0              0.0    1.0         1.0            0.0   \n",
              "11            0.0              0.0    0.0         0.0            1.0   \n",
              "12            1.0              0.0    0.0         0.0            1.0   \n",
              "13            0.0              0.0    1.0         1.0            0.0   \n",
              "14            0.0              0.0    1.0         1.0            0.0   \n",
              "\n",
              "    is_continuous1  slantness1  tilt1  entry_stroke_a1  staff_of_a1  \\\n",
              "0              0.0         0.0    0.0              0.0          0.0   \n",
              "1              0.0         0.0    0.0              0.0          0.0   \n",
              "2              0.0         0.0    0.0              0.0          0.0   \n",
              "3              0.0         0.0    0.0              0.0          0.0   \n",
              "4              1.0         0.0    0.0              0.0          1.0   \n",
              "5              0.0         0.0    0.0              1.0          0.0   \n",
              "6              0.0         0.0    1.0              0.0          0.0   \n",
              "7              0.0         1.0    0.0              0.0          0.0   \n",
              "8              1.0         0.0    0.0              0.0          0.0   \n",
              "9              0.0         0.0    0.0              0.0          0.0   \n",
              "10             0.0         0.0    0.0              0.0          1.0   \n",
              "11             1.0         0.0    0.0              0.0          0.0   \n",
              "12             1.0         0.0    0.0              0.0          0.0   \n",
              "13             0.0         0.0    0.0              0.0          1.0   \n",
              "14             0.0         0.0    0.0              0.0          1.0   \n",
              "\n",
              "    formation_n1  staff_of_d1  exit_stroke_d1  word_formation1  constancy1  \n",
              "0            0.0          0.0             1.0              0.0         0.0  \n",
              "1            0.0          0.0             0.0              0.0         0.0  \n",
              "2            1.0          0.0             0.0              1.0         1.0  \n",
              "3            1.0          0.0             0.0              1.0         1.0  \n",
              "4            0.0          1.0             1.0              0.0         0.0  \n",
              "5            0.0          1.0             1.0              0.0         0.0  \n",
              "6            0.0          0.0             0.0              0.0         0.0  \n",
              "7            0.0          0.0             0.0              0.0         0.0  \n",
              "8            0.0          0.0             0.0              0.0         0.0  \n",
              "9            1.0          0.0             0.0              1.0         1.0  \n",
              "10           0.0          0.0             0.0              1.0         1.0  \n",
              "11           0.0          0.0             1.0              0.0         0.0  \n",
              "12           0.0          1.0             0.0              0.0         0.0  \n",
              "13           1.0          0.0             0.0              0.0         1.0  \n",
              "14           1.0          0.0             0.0              1.0         0.0  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "metadata": {
        "id": "7B3QnTog43r1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Task 2"
      ]
    },
    {
      "metadata": {
        "id": "_rS_6-Ag4fsz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Bayesian Models"
      ]
    },
    {
      "metadata": {
        "id": "RIj5ENfc_x2l",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        " Model 1"
      ]
    },
    {
      "metadata": {
        "id": "IkqIWa82jMZH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "structure1_2=[('pen_pressure2','letter_spacing2'),('dimension2', 'size2'),('size2','slantness2'),('is_lowercase2', 'staff_of_a2'),('is_continuous2', 'exit_stroke_d2'),('is_continuous2', 'entry_stroke_a2'),('is_continuous2', 'is_lowercase2'),('slantness2', 'tilt2'),('formation_n2', 'constancy2'), ('formation_n2', 'word_formation2'), ('formation_n2', 'dimension2'),('formation_n2', 'size2'), ('formation_n2', 'staff_of_a2'),('staff_of_d2', 'is_continuous2'), ('staff_of_d2', 'exit_stroke_d2'), ('staff_of_d2', 'is_lowercase2'),('word_formation2', 'dimension2'), ('word_formation2', 'staff_of_a2'), ('word_formation2', 'size2'),('word_formation2', 'constancy2'), ('constancy2', 'staff_of_a2'),('constancy2', 'dimension2'),('constancy2', 'size2'),('pen_pressure2', 'exit_stroke_d2')]\n",
        "structure1_1=[('pen_pressure1','letter_spacing1'),('dimension1', 'size1'),('size1','slantness1'),('is_lowercase1', 'staff_of_a1'),('is_continuous1', 'exit_stroke_d1'),('is_continuous1', 'entry_stroke_a1'),('is_continuous1', 'is_lowercase1'),('slantness1', 'tilt1'),('formation_n1', 'constancy1'), ('formation_n1', 'word_formation1'), ('formation_n1', 'dimension1'),('formation_n1', 'size1'), ('formation_n1', 'staff_of_a1'),('staff_of_d1', 'is_continuous1'), ('staff_of_d1', 'exit_stroke_d1'), ('staff_of_d1', 'is_lowercase1'),('word_formation1', 'dimension1'), ('word_formation1', 'staff_of_a1'), ('word_formation1', 'size1'),('word_formation1', 'constancy1'), ('constancy1', 'staff_of_a1'),('constancy1', 'dimension1'),('constancy1', 'size1'),('pen_pressure1', 'exit_stroke_d1')]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rqaIb9n2_0Yf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Model 2"
      ]
    },
    {
      "metadata": {
        "id": "eCqAx5rvAFfo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "structure2_1=[('pen_pressure1', 'is_lowercase1'), ('pen_pressure1', 'letter_spacing1'), ('size1', 'slantness1'), ('size1', 'pen_pressure1'), ('size1', 'staff_of_d1'), ('size1', 'letter_spacing1'), ('size1', 'exit_stroke_d1'), ('size1', 'entry_stroke_a1'), ('dimension1', 'size1'), ('dimension1', 'is_continuous1'), ('dimension1', 'slantness1'), ('dimension1', 'pen_pressure1'), ('is_lowercase1', 'staff_of_a1'), ('is_lowercase1', 'exit_stroke_d1'), ('is_continuous1', 'exit_stroke_d1'), ('is_continuous1', 'letter_spacing1'), ('is_continuous1', 'entry_stroke_a1'), ('is_continuous1', 'staff_of_a1'), ('is_continuous1', 'is_lowercase1'), ('slantness1', 'is_continuous1'), ('slantness1', 'tilt1'), ('entry_stroke_a1', 'pen_pressure1'), ('formation_n1', 'constancy1'), ('formation_n1', 'word_formation1'), ('formation_n1', 'dimension1'), ('formation_n1', 'staff_of_d1'), ('formation_n1', 'is_continuous1'), ('formation_n1', 'size1'), ('formation_n1', 'staff_of_a1'), ('staff_of_d1', 'is_continuous1'), ('staff_of_d1', 'exit_stroke_d1'), ('staff_of_d1', 'is_lowercase1'), ('staff_of_d1', 'slantness1'), ('staff_of_d1', 'entry_stroke_a1'), ('word_formation1', 'dimension1'), ('word_formation1', 'staff_of_a1'), ('word_formation1', 'size1'), ('word_formation1', 'staff_of_d1'), ('word_formation1', 'constancy1'), ('constancy1', 'staff_of_a1'), ('constancy1', 'letter_spacing1'), ('constancy1', 'dimension1')]\n",
        "structure2_2=[('pen_pressure2', 'is_lowercase2'), ('pen_pressure2', 'letter_spacing2'), ('size2', 'slantness2'), ('size2', 'pen_pressure2'), ('size2', 'staff_of_d2'), ('size2', 'letter_spacing2'), ('size2', 'exit_stroke_d2'), ('size2', 'entry_stroke_a2'), ('dimension2', 'size2'), ('dimension2', 'is_continuous2'), ('dimension2', 'slantness2'), ('dimension2', 'pen_pressure2'), ('is_lowercase2', 'staff_of_a2'), ('is_lowercase2', 'exit_stroke_d2'), ('is_continuous2', 'exit_stroke_d2'), ('is_continuous2', 'letter_spacing2'), ('is_continuous2', 'entry_stroke_a2'), ('is_continuous2', 'staff_of_a2'), ('is_continuous2', 'is_lowercase2'), ('slantness2', 'is_continuous2'), ('slantness2', 'tilt2'), ('entry_stroke_a2', 'pen_pressure2'), ('formation_n2', 'constancy2'), ('formation_n2', 'word_formation2'), ('formation_n2', 'dimension2'), ('formation_n2', 'staff_of_d2'), ('formation_n2', 'is_continuous2'), ('formation_n2', 'size2'), ('formation_n2', 'staff_of_a2'), ('staff_of_d2', 'is_continuous2'), ('staff_of_d2', 'exit_stroke_d2'), ('staff_of_d2', 'is_lowercase2'), ('staff_of_d2', 'slantness2'), ('staff_of_d2', 'entry_stroke_a2'), ('word_formation2', 'dimension2'), ('word_formation2', 'staff_of_a2'), ('word_formation2', 'size2'), ('word_formation2', 'staff_of_d2'), ('word_formation2', 'constancy2'), ('constancy2', 'staff_of_a2'), ('constancy2', 'letter_spacing2'), ('constancy2', 'dimension2')]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bmQTdsx8_7-9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Model 3 "
      ]
    },
    {
      "metadata": {
        "id": "el8Y1BlV_7wV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "structure3_1=[('pen_pressure1', 'is_lowercase1'),  ('size1', 'slantness1'), ('size1', 'pen_pressure1'),  ('size1', 'letter_spacing1'), ('size1', 'exit_stroke_d1'), ('size1', 'entry_stroke_a1'), ('dimension1', 'size1'), ('dimension1', 'is_continuous1'),  ('dimension1', 'pen_pressure1'), ('is_lowercase1', 'staff_of_a1'), ('is_lowercase1', 'exit_stroke_d1'), ('is_continuous1', 'exit_stroke_d1'),  ('is_continuous1', 'entry_stroke_a1'), ('is_continuous1', 'staff_of_a1'), ('is_continuous1', 'is_lowercase1'), ('slantness1', 'is_continuous1'), ('slantness1', 'tilt1'),  ('formation_n1', 'constancy1'), ('formation_n1', 'word_formation1'), ('formation_n1', 'dimension1'), ('formation_n1', 'staff_of_d1'), ('formation_n1', 'is_continuous1'), ('formation_n1', 'size1'), ('formation_n1', 'staff_of_a1'), ('staff_of_d1', 'is_continuous1'), ('staff_of_d1', 'exit_stroke_d1'), ('staff_of_d1', 'is_lowercase1'), ('staff_of_d1', 'slantness1'), ('staff_of_d1', 'entry_stroke_a1'), ('word_formation1', 'dimension1'), ('word_formation1', 'staff_of_a1'), ('word_formation1', 'size1'), ('word_formation1', 'staff_of_d1'), ('word_formation1', 'constancy1'), ('constancy1', 'staff_of_a1'), ('constancy1', 'letter_spacing1'), ('constancy1', 'dimension1')]\n",
        "structure3_2=[('pen_pressure2', 'is_lowercase2'),  ('size2', 'slantness2'), ('size2', 'pen_pressure2'),  ('size2', 'letter_spacing2'), ('size2', 'exit_stroke_d2'), ('size2', 'entry_stroke_a2'), ('dimension2', 'size2'), ('dimension2', 'is_continuous2'),  ('dimension2', 'pen_pressure2'), ('is_lowercase2', 'staff_of_a2'), ('is_lowercase2', 'exit_stroke_d2'), ('is_continuous2', 'exit_stroke_d2'),  ('is_continuous2', 'entry_stroke_a2'), ('is_continuous2', 'staff_of_a2'), ('is_continuous2', 'is_lowercase2'), ('slantness2', 'is_continuous2'), ('slantness2', 'tilt2'),  ('formation_n2', 'constancy2'), ('formation_n2', 'word_formation2'), ('formation_n2', 'dimension2'), ('formation_n2', 'staff_of_d2'), ('formation_n2', 'is_continuous2'), ('formation_n2', 'size2'), ('formation_n2', 'staff_of_a2'), ('staff_of_d2', 'is_continuous2'), ('staff_of_d2', 'exit_stroke_d2'), ('staff_of_d2', 'is_lowercase2'), ('staff_of_d2', 'slantness2'), ('staff_of_d2', 'entry_stroke_a2'), ('word_formation2', 'dimension2'), ('word_formation2', 'staff_of_a2'), ('word_formation2', 'size2'), ('word_formation2', 'staff_of_d2'), ('word_formation2', 'constancy2'), ('constancy2', 'staff_of_a2'), ('constancy2', 'letter_spacing2'), ('constancy2', 'dimension2')]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "s133tS7O_62U",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Model 4"
      ]
    },
    {
      "metadata": {
        "id": "aWIIouoJ_6il",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "structure4_1=[('pen_pressure1', 'is_lowercase1'), ('pen_pressure1', 'letter_spacing1'), ('size1', 'slantness1'),  ('size1', 'staff_of_d1'), ('size1', 'letter_spacing1'), ('size1', 'exit_stroke_d1'), ('size1', 'entry_stroke_a1'), ('dimension1', 'size1'), ('dimension1', 'is_continuous1'), ('dimension1', 'slantness1'), ('dimension1', 'pen_pressure1'),  ('is_lowercase1', 'exit_stroke_d1'), ('is_continuous1', 'exit_stroke_d1'), ('is_continuous1', 'letter_spacing1'), ('is_continuous1', 'entry_stroke_a1'), ('is_continuous1', 'staff_of_a1'), ('is_continuous1', 'is_lowercase1'), ('slantness1', 'is_continuous1'), ('slantness1', 'tilt1'), ('entry_stroke_a1', 'pen_pressure1'), ('formation_n1', 'constancy1'), ('formation_n1', 'word_formation1'), ('formation_n1', 'dimension1'), ('formation_n1', 'staff_of_d1'), ('formation_n1', 'is_continuous1'), ('formation_n1', 'size1'), ('formation_n1', 'staff_of_a1'), ('staff_of_d1', 'is_continuous1'), ('staff_of_d1', 'exit_stroke_d1'), ('staff_of_d1', 'is_lowercase1'), ('staff_of_d1', 'slantness1'), ('staff_of_d1', 'entry_stroke_a1'), ('word_formation1', 'dimension1'), ('word_formation1', 'staff_of_a1'), ('word_formation1', 'size1'), ('word_formation1', 'staff_of_d1'), ('word_formation1', 'constancy1'), ('constancy1', 'staff_of_a1'), ('constancy1', 'letter_spacing1'), ('constancy1', 'dimension1')]\n",
        "structure4_2=[('pen_pressure2', 'is_lowercase2'), ('pen_pressure2', 'letter_spacing2'), ('size2', 'slantness2'),  ('size2', 'staff_of_d2'), ('size2', 'letter_spacing2'), ('size2', 'exit_stroke_d2'), ('size2', 'entry_stroke_a2'), ('dimension2', 'size2'), ('dimension2', 'is_continuous2'), ('dimension2', 'slantness2'), ('dimension2', 'pen_pressure2'),  ('is_lowercase2', 'exit_stroke_d2'), ('is_continuous2', 'exit_stroke_d2'), ('is_continuous2', 'letter_spacing2'), ('is_continuous2', 'entry_stroke_a2'), ('is_continuous2', 'staff_of_a2'), ('is_continuous2', 'is_lowercase2'), ('slantness2', 'is_continuous2'), ('slantness2', 'tilt2'), ('entry_stroke_a2', 'pen_pressure2'), ('formation_n2', 'constancy2'), ('formation_n2', 'word_formation2'), ('formation_n2', 'dimension2'), ('formation_n2', 'staff_of_d2'), ('formation_n2', 'is_continuous2'), ('formation_n2', 'size2'), ('formation_n2', 'staff_of_a2'), ('staff_of_d2', 'is_continuous2'), ('staff_of_d2', 'exit_stroke_d2'), ('staff_of_d2', 'is_lowercase2'), ('staff_of_d2', 'slantness2'), ('staff_of_d2', 'entry_stroke_a2'), ('word_formation2', 'dimension2'), ('word_formation2', 'staff_of_a2'), ('word_formation2', 'size2'), ('word_formation2', 'staff_of_d2'), ('word_formation2', 'constancy2'), ('constancy2', 'staff_of_a2'), ('constancy2', 'letter_spacing2'), ('constancy2', 'dimension2')]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2o3rdmwhL1Cz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# find the k2 of the given data\n",
        "k2=K2Score(seen_train)\n",
        "# Perform Hill Climb Search on the dataset\n",
        "#hc=HillClimbSearch(features,scoring_method=k2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "udcMwyWYL18Z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Predict the best model\n",
        "#best_model=hc.estimate()\n",
        "#print(best_model.edges())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UGU921IeOH5v",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def create_model(Train_data,s):\n",
        "  if s==1:\n",
        "    structure1=structure1_1\n",
        "    structure2=structure1_2\n",
        "  elif s==2:\n",
        "    structure1=structure2_1\n",
        "    structure2=structure2_2\n",
        "  elif s==3:\n",
        "    structure1=structure3_1\n",
        "    structure2=structure3_2\n",
        "  elif s==4:\n",
        "    structure1=structure4_1\n",
        "    structure2=structure4_2\n",
        "  # Create Bayesian Model\n",
        "  model=BayesianModel()\n",
        "  # add edges to bayesian model\n",
        "  model.add_edges_from(structure1)\n",
        "  # replicating structure\n",
        "  model.add_edges_from(structure2)\n",
        "  # adding validation node\n",
        "  model.add_edges_from([('pen_pressure1','label'),('pen_pressure2','label'),('tilt1','label'),('tilt2','label'),\n",
        "                        ('staff_of_d1','label'),('staff_of_d2','label'),('formation_n1','label'),('formation_n2','label'),\n",
        "                        ('exit_stroke_d1','label'),('exit_stroke_d2','label'),('is_continuous1','label'),('is_continuous2','label')])\n",
        "  #Train bayesian model on Train data\n",
        "  model.fit(Train_data)\n",
        "  print('K2 Score:'+str(k2.score(model)))\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NPda_94TMGmD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Test the model"
      ]
    },
    {
      "metadata": {
        "id": "wk_5o5r4YiJg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def predict(model,data):\n",
        "  '''total=len(data)\n",
        "  predict_data=data.copy()\n",
        "  predict_data.drop('label',axis=1,inplace=True)\n",
        "  predict_data=predict_data.iloc[:total,:]\n",
        "  predict_data=predict_data-1\n",
        "  y=model.predict(predict_data)\n",
        "  #predict_data\n",
        "  label=data.iloc[:total,30].copy()\n",
        "  accu=0\n",
        "  for i, j in zip(y.values,label.values):\n",
        "    if i==j:\n",
        "      accu+=1'''\n",
        "  inference=VariableElimination(model)\n",
        "  accu=0\n",
        "  data=data.sample(frac=1).reset_index(drop=True)\n",
        "  lenght=data.shape[0]\n",
        "  for row in range(0,lenght):\n",
        "    actual=data.iloc[row,30]\n",
        "    #### Predict right\n",
        "    #print('predict right')\n",
        "    # actual labels\n",
        "    label=data.iloc[row,15:30].copy()\n",
        "    # feature of left image\n",
        "    all_feature=data.iloc[row,0:15].copy()\n",
        "    all_feature=all_feature.to_dict()\n",
        "    # subtracting 1 from all 15 features \n",
        "    for e in feature_nodes[:15]:\n",
        "      all_feature[e]=all_feature[e]-1\n",
        "    # label=0\n",
        "    all_feature['label']=0\n",
        "    #print('label 0:')\n",
        "    label0=1\n",
        "    #start Time\n",
        "    #start = timer()\n",
        "    phi_query_right_0=inference.query(feature_nodes[15:30],evidence=all_feature)#['label']\n",
        "    for e in feature_nodes[15:30]:\n",
        "      #print(phi_query[e])\n",
        "      label0=label0*phi_query_right_0[e].values[label[e]-1]\n",
        "    # label =1\n",
        "    # label=0\n",
        "    #print('label 1:')\n",
        "    label1=1\n",
        "    all_feature['label']=1\n",
        "    #start Time\n",
        "    #start = timer()\n",
        "    phi_query_right_1=inference.query(feature_nodes[15:30],evidence=all_feature)#['label']\n",
        "    for e in feature_nodes[15:30]:\n",
        "      #print(phi_query[e])\n",
        "      label1=label1*phi_query_right_1[e].values[label[e]-1]\n",
        "    #print(all_feature)\n",
        "    d1=(label0/label1)\n",
        "    d1=math.log(d1)\n",
        "    #if d1>=0:\n",
        "      #predicted=0\n",
        "    #else:\n",
        "      #predicted=1\n",
        "    #print(predicted,actual)\n",
        "    #if predicted==actual:\n",
        "      #accu+=1\n",
        "  #print('accu:'+str(accu/lenght))\n",
        "  \n",
        "  \n",
        "    #### Predict left\n",
        "    print('predict left')\n",
        "    # actual labels\n",
        "    label=data.iloc[row,0:15].copy()\n",
        "    # feature of left image\n",
        "    all_feature=data.iloc[row,15:30].copy()\n",
        "    all_feature=all_feature.to_dict()\n",
        "    # subtracting 1 from all 15 features \n",
        "    for e in feature_nodes[15:30]:\n",
        "      all_feature[e]=all_feature[e]-1\n",
        "    # label=0\n",
        "    all_feature['label']=0\n",
        "    print('label 0:')\n",
        "    label0=1\n",
        "    #start Time\n",
        "    #start = timer()\n",
        "    phi_query_left_0=inference.query(feature_nodes[0:15],evidence=all_feature)#['label']\n",
        "    for e in feature_nodes[0:15]:\n",
        "      #print(phi_query[e])\n",
        "      label0=label0*phi_query_left_0[e].values[label[e]-1]\n",
        "    # label =1\n",
        "    # label=0\n",
        "    print('label 1:')\n",
        "    label1=1\n",
        "    all_feature['label']=1\n",
        "    #start Time\n",
        "    #start = timer()\n",
        "    phi_query_left_1=inference.query(feature_nodes[0:15],evidence=all_feature)#['label']\n",
        "    for e in feature_nodes[0:15]:\n",
        "      #print(phi_query[e])\n",
        "      label1=label1*phi_query_left_1[e].values[label[e]-1]\n",
        "    d2=(label0/label1)\n",
        "    d2=math.log(d2)\n",
        "    d=(d1+d2)/2\n",
        "    if d>=0:\n",
        "      predicted=0\n",
        "    else:\n",
        "      predicted=1\n",
        "    print(predicted,actual)\n",
        "    if predicted==actual:\n",
        "      accu+=1\n",
        "  print('accu:'+str(accu))\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "96CkEK0uLByV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Concatinating dataset"
      ]
    },
    {
      "metadata": {
        "id": "Q3aNcZJQ4eid",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def join_features(Train,name):\n",
        "  #loop for number of rows in dataset\n",
        "  v=[]\n",
        "  for i in range(0,len(Train.index)):\n",
        "    try:\n",
        "      #extract features for left\n",
        "      x=features.loc[Train.iloc[i,0],:]\n",
        "      #extract features for right\n",
        "      y=features.loc[Train.iloc[i,1],:]\n",
        "      #extract label\n",
        "      w=Train.iloc[i,2]\n",
        "      #concat all 3 side by side\n",
        "      z=np.hstack([x,y,w])\n",
        "      # add values to list\n",
        "      v.append(z)\n",
        "    except:\n",
        "      continue\n",
        "  len(v)\n",
        "  # convert into dataframe\n",
        "  v=pd.DataFrame(v)\n",
        "  # rename columns to dataframe\n",
        "  v.columns=all_nodes\n",
        "  # write into csv\n",
        "  v.to_csv(name+\".csv\",sep=\",\",index=False)\n",
        "  #files.download(name+\".csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KrIbfKKZRtL2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# SeenDataset"
      ]
    },
    {
      "metadata": {
        "id": "L6gs8qVZRsYF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#read dataset\n",
        "Train_seen_data=pd.read_csv(\"Dataset/AML_Proj2/seen-dataset/dataset_seen_training_siamese.csv\")\n",
        "# drop the unwanted column\n",
        "#Train_data=Train_data.drop(\"number\",axis=1)\n",
        "#Train_data=Train_data.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "\n",
        "#Train_seen_data.set_index('number',inplace=True)\n",
        "#join_features(Train_seen_data,\"31_Seen_Train\")\n",
        "\n",
        "#read dataset\n",
        "Val_seen_data=pd.read_csv(\"Dataset/AML_Proj2/seen-dataset/dataset_seen_validation_siamese.csv\")\n",
        "# drop the unwanted column\n",
        "#Val_data=Val_data.drop(\"number\",axis=1)\n",
        "#Val_data=Val_data.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "#Val_seen_data.set_index('number',inplace=True)\n",
        "#join_features(Train_seen_data,\"31_Seen_Val\")\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_jmYtyn8USkI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "32c252e5-5e96-41c3-c99d-374e93f02c5f"
      },
      "cell_type": "code",
      "source": [
        "len(Val_seen_data)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "906"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "metadata": {
        "id": "1jkdgQUA3L-e",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "31535332-5422-4018-9194-3356537b1586"
      },
      "cell_type": "code",
      "source": [
        "seen_train=pd.read_csv(\"Dataset/AML_Proj2/31_Seen_Train.csv\")\n",
        "seen_val=pd.read_csv(\"Dataset/AML_Proj2/31_Seen_Val.csv\")\n",
        "len(seen_val)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "894"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "metadata": {
        "id": "MtS_tku4g3Gc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "749d593e-a498-4f41-c7c2-147b132a7873"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "start = timeit.default_timer()\n",
        "model=create_model(seen_train,1)\n",
        "\n",
        "stop = timeit.default_timer()\n",
        "\n",
        "print('Time: ', stop - start) "
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "K2 Score:-2256601.812071786\n",
            "Time:  375.6305282039975\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "R_YhBFrH382k",
        "colab_type": "code",
        "outputId": "bd45b390-d508-4bef-ed24-23163afe8ab5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "predict(model,seen_val)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accu: 60.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Wu_fQg_IR4wf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Unseen Dataset"
      ]
    },
    {
      "metadata": {
        "id": "FTu9gR-kSisV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#read dataset\n",
        "Train_unseen_data=pd.read_csv(\"Dataset/AML_Proj2/unseen-dataset/dataset_seen_training_siamese.csv\")\n",
        "# drop the unwanted column\n",
        "#Train_data=Train_data.drop(\"number\",axis=1)\n",
        "#Train_data=Train_data.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "\n",
        "#Train_unseen_data.set_index('number',inplace=True)\n",
        "#join_features(Train_unseen_data,\"31_unSeen_Train\")\n",
        "\n",
        "#read dataset\n",
        "Val_unseen_data=pd.read_csv(\"Dataset/AML_Proj2/unseen-dataset/dataset_seen_validation_siamese.csv\")\n",
        "# drop the unwanted column\n",
        "#Val_data=Val_data.drop(\"number\",axis=1)\n",
        "#Val_data=Val_data.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "#Val_unseen_data.set_index('number',inplace=True)\n",
        "#join_features(Train_unseen_data,\"31_unSeen_Val\")\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3b-ileyY3Nia",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "unseen_train=pd.read_csv(\"Dataset/AML_Proj2/31_unSeen_Train.csv\")\n",
        "unseen_val=pd.read_csv(\"Dataset/AML_Proj2/31_unSeen_Val.csv\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "r2EGnm9vg4x4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "fb082f80-fcb7-4c86-d9c5-3ce13c62bc42"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "start = timeit.default_timer()\n",
        "\n",
        "model=create_model(unseen_train,1)\n",
        "\n",
        "stop = timeit.default_timer()\n",
        "\n",
        "print('Time: ', stop - start)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "K2 Score:-2256601.812071786\n",
            "Time:  374.97740566300126\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0CTLMX3KVjIU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7a23cdb6-1330-444c-c81e-6e675ed25dd3"
      },
      "cell_type": "code",
      "source": [
        "predict(model,unseen_val)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accu: 54.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "KnPYCYFlSj8z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Shuffled Dataset"
      ]
    },
    {
      "metadata": {
        "id": "g6aN-_s3SmNU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#read dataset\n",
        "Train_shuffled_data=pd.read_csv(\"Dataset/AML_Proj2/shuffled-dataset/dataset_seen_training_siamese.csv\")\n",
        "# drop the unwanted column\n",
        "#Train_data=Train_data.drop(\"number\",axis=1)\n",
        "#Train_data=Train_data.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "#Train_shuffled_data.set_index('number',inplace=True)\n",
        "#join_features(Train_shuffled_data,\"31_shuffled_Train\")\n",
        "\n",
        "#read dataset\n",
        "Val_shuffled_data=pd.read_csv(\"Dataset/AML_Proj2/shuffled-dataset/dataset_seen_validation_siamese.csv\")\n",
        "# drop the unwanted column\n",
        "#Val_data=Val_data.drop(\"number\",axis=1)\n",
        "#Val_data=Val_data.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "#Val_shuffled_data.set_index('number',inplace=True)\n",
        "#join_features(Train_shuffled_data,\"31_shuffled_Val\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NaH3q0wc3O19",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "shuffled_train=pd.read_csv(\"Dataset/AML_Proj2/31_shuffled_Train.csv\")\n",
        "shuffled_val=pd.read_csv(\"Dataset/AML_Proj2/31_shuffled_Val.csv\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kQLrbTsgg6Ne",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "6a87b3bf-8b12-40fd-f696-be4af440ff58"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "start = timeit.default_timer()\n",
        "model=create_model(shuffled_train,4)\n",
        "\n",
        "stop = timeit.default_timer()\n",
        "\n",
        "print('Time: ', stop - start)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "K2 Score:-2069727.6393613645\n",
            "Time:  382.2711251569999\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "IkxQeez5VlUx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "57bd127b-de05-4248-8b90-6d36b80799eb"
      },
      "cell_type": "code",
      "source": [
        "predict(model,shuffled_val)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accu: 55.7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9kAMeki0BVKf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Task 3"
      ]
    },
    {
      "metadata": {
        "id": "zVSCiMLPBYXJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Siamese Network"
      ]
    },
    {
      "metadata": {
        "id": "2agrqnmy2fxJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "imDim = 64\n",
        "input_shape  = (imDim,imDim,1)\n",
        "inp_img = Input(shape = (imDim,imDim,1), name = 'ImageInput')\n",
        "model = inp_img\n",
        "\n",
        "#     model = Input(shape=(imDim,imDim,1))\n",
        "#     model.add(Input(shape = (imDim,imDim,1), name = 'FeatureNet_ImageInput'))\n",
        "model = Conv2D(32,kernel_size=(3, 3),activation='relu',input_shape=input_shape,padding='valid')(model)\n",
        "\n",
        "#####normalization\n",
        "#model= Dropout(0.25)(model)\n",
        "\n",
        "#     model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
        "model = MaxPooling2D((2,2), padding='valid')(model)\n",
        "model = Conv2D(64, (3, 3), activation='relu',padding='valid')(model)\n",
        "#     model.add(Conv2D(32, (3, 3), activation='relu',padding='same'))\n",
        "model = MaxPooling2D((2,2),padding='valid')(model)\n",
        "\n",
        "#####normalization\n",
        "#model= BatchNormalization()(model)\n",
        "\n",
        "#     model.add(Conv2D(16, (3, 3), activation='relu',padding='same'))\n",
        "model = Conv2D(128, (3, 3), activation='relu',padding='valid')(model)\n",
        "model = MaxPooling2D((2,2),padding='valid')(model)\n",
        "#     model.add(Conv2D(1, (3, 3), activation='relu',padding='same'))\n",
        "#     model.add(Conv2D(2, (3, 3), activation='relu',padding='same'))\n",
        "\n",
        "model = Conv2D(256, (1, 1), activation='relu',padding='valid')(model)\n",
        "model = MaxPooling2D((2,2),padding='valid')(model)\n",
        "\n",
        "model = Conv2D(64, (1, 1), activation='relu',padding='valid')(model)\n",
        "# model = MaxPooling2D((2,2),padding='valid')(model)\n",
        "model = Flatten()(model)\n",
        "\n",
        "# img_in = np.array((-1,imDim,imDim,1), dtype='float32')\n",
        "# img_in = tf.placeholder(shape=(imDim,imDim,1), dtype='float32')\n",
        "\n",
        "feat = Model(inputs=[inp_img], outputs=[model],name = 'Feat_Model')\n",
        "feat.summary()\n",
        "\n",
        "left_img = Input(shape = (imDim,imDim,1), name = 'left_img')\n",
        "right_img = Input(shape = (imDim,imDim,1), name = 'right_img')\n",
        "\n",
        "left_feats = feat(left_img)\n",
        "right_feats = feat(right_img)\n",
        "merged_feats = concatenate([left_feats, right_feats], name = 'concat_feats')\n",
        "merged_feats = Dense(1024, activation = 'linear')(merged_feats)\n",
        "merged_feats=Dropout(0.5)(merged_feats)\n",
        "#merged_feats = BatchNormalization()(merged_feats)\n",
        "merged_feats = Activation('relu')(merged_feats)\n",
        "merged_feats = Dense(4, activation = 'linear')(merged_feats)\n",
        "merged_feats=Dropout(0.2)(merged_feats)\n",
        "#merged_feats = BatchNormalization()(merged_feats)\n",
        "merged_feats = Activation('relu')(merged_feats)\n",
        "merged_feats = Dense(1, activation = 'sigmoid')(merged_feats)\n",
        "similarity_model = Model(inputs = [left_img, right_img], outputs = [merged_feats], name = 'Similarity_Model')\n",
        "similarity_model.summary()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Yz9qS0cInyfI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "similarity_model.compile(optimizer=optimizers.adadelta(lr=0.001),loss=\"binary_crossentropy\",metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fnJ7Dw4tsE6t",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def image_dataset(data):\n",
        "  Train_dataset={}\n",
        "  Val_dataset={}\n",
        "  #getting inside directory\n",
        "  imgs=os.listdir(\"Dataset/AML_Proj2/\"+data+\"/TrainingSet/\")\n",
        "  #looping over all the images in the directory\n",
        "  for name in imgs:\n",
        "    img=cv2.imread(\"Dataset/AML_Proj2/\"+data+\"/TrainingSet/\"+str(name),0)\n",
        "    if img is not None:\n",
        "      img=(255-img)/255\n",
        "      Train_dataset[name]=img\n",
        "  imgs=os.listdir(\"Dataset/AML_Proj2/\"+data+\"/ValidationSet/\")\n",
        "  #looping over all the images in the directory\n",
        "  for name in imgs:\n",
        "    img=cv2.imread(\"Dataset/AML_Proj2/\"+data+\"/ValidationSet/\"+str(name),0)\n",
        "    if img is not None:\n",
        "      img=(255-img)/255\n",
        "      Val_dataset[name]=img\n",
        "  return Train_dataset,Val_dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YRN2dtUTytQ1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Image Dataset"
      ]
    },
    {
      "metadata": {
        "id": "fChS0EH3yxbN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "Train_seen_images,Val_seen_images=image_dataset(\"seen-dataset\")\n",
        "Train_unseen_images,Val_unseen_images=image_dataset(\"unseen-dataset\")\n",
        "Train_shuffled_images,Val_shuffled_images=image_dataset(\"shuffled-dataset\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YrYka3mgyyO0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Data Generation"
      ]
    },
    {
      "metadata": {
        "id": "cj7YNsZIsF1_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_batches(start,end,batch_size,s):\n",
        "  w=h=64\n",
        "  if s=='seen_train':\n",
        "    X=Train_seen_data\n",
        "    dataset=Train_seen_images\n",
        "  elif s==\"seen_val\":\n",
        "    X=Val_seen_data\n",
        "    dataset=Val_seen_images  \n",
        "  elif s==\"unseen_train\":\n",
        "    X=Train_unseen_data\n",
        "    dataset=Train_unseen_images\n",
        "  elif s==\"unseen_val\":\n",
        "    X=Val_unseen_data\n",
        "    dataset=Val_unseen_images\n",
        "  elif s==\"shuffled_train\":\n",
        "    X=Train_shuffled_data\n",
        "    dataset=Train_shuffled_images\n",
        "  elif s==\"shuffled_val\":\n",
        "    X=Val_shuffled_data\n",
        "    dataset=Val_shuffled_images\n",
        "  X=X.sample(frac=1).reset_index(drop=True)\n",
        "  # create list for pair images\n",
        "  pairs=[np.zeros((batch_size,h,w,1)) for i in range(2)]\n",
        "  # target of the pair\n",
        "  targets=np.zeros((batch_size))\n",
        "  # find min value for end\n",
        "  end=min(len(Train_seen_data),end)\n",
        "  # loop for each batch\n",
        "  for i,j in zip(range(start,end),range(batch_size)):\n",
        "    try:\n",
        "      # storing left image in pair\n",
        "      pairs[0][j,:,:,:]=dataset[X.iloc[i,0]].reshape(w,h,1)\n",
        "      # storing right image in pair\n",
        "      pairs[1][j,:,:,:]=dataset[X.iloc[i,1]].reshape(w,h,1)\n",
        "      # storing target\n",
        "      targets[j]=X.iloc[i,2]\n",
        "    except:\n",
        "      continue\n",
        "  #print(pairs[0][0])\n",
        "  return pairs,targets"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MB34YEprS5Qw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def generate(batch_size,s):\n",
        "  start=0\n",
        "  end=batch_size\n",
        "  while True:\n",
        "    pairs, targets=get_batches(start,end,batch_size,s)\n",
        "    yield(pairs,targets)\n",
        "    start=end\n",
        "    end=end+batch_size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "X_adjUrz1f7a",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Seen Evaluation"
      ]
    },
    {
      "metadata": {
        "id": "8eiWdvT5aiD1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "batch_size=64\n",
        "steps_per_epoch=math.ceil(len(Train_seen_data)/batch_size)\n",
        "vsteps_per_epoch=math.ceil(len(Val_seen_data)/batch_size)\n",
        "similarity_model.fit_generator(generate(batch_size,\"seen_train\"),epochs=10,steps_per_epoch=steps_per_epoch,validation_data=generate(batch_size,\"seen_val\"),validation_steps=vsteps_per_epoch)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "e6bHrKg51mGe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Unseen Evaluation"
      ]
    },
    {
      "metadata": {
        "id": "2OmTBli21tZ3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "batch_size=64\n",
        "steps_per_epoch=math.ceil(len(Train_seen_data)/batch_size)\n",
        "vsteps_per_epoch=math.ceil(len(Val_seen_data)/batch_size)\n",
        "similarity_model.fit_generator(generate(batch_size,\"unseen_train\"),epochs=10,steps_per_epoch=steps_per_epoch,validation_data=generate(batch_size,\"unseen_val\"),validation_steps=vsteps_per_epoch)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dwP9eu9-1uM_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#  Shuffled Evaluation"
      ]
    },
    {
      "metadata": {
        "id": "cRjHrMDh14cF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "batch_size=64\n",
        "steps_per_epoch=math.ceil(len(Train_seen_data)/batch_size)\n",
        "vsteps_per_epoch=math.ceil(len(Val_seen_data)/batch_size)\n",
        "similarity_model.fit_generator(generate(batch_size,\"shuffled_train\"),epochs=10,steps_per_epoch=steps_per_epoch,validation_data=generate(batch_size,\"shuffled_val\"),validation_steps=vsteps_per_epoch)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}